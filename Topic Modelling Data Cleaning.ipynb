{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling - Data cleaning\n",
    "\n",
    "We have downloaded some speeches by B. Obama and M. Romney made during US Presidential election 2012. The speeches are uploaded on GitHUB and we would try to find some imortant topics each of their speeches consisted of. We will do Topic modelling using Gensim and visualize these topics using a wonderful package called pyLDAvis.\n",
    "\n",
    "In this notebook - we will concentrate on cleaning the text which is very important steps towards modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find folders and text under a given path\n",
    "import os\n",
    "\n",
    "# NLTK package\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text using spacy\n",
    "\n",
    "# 1. Word Tokenize text\n",
    "# 2. Ignore stopwords and non alpha characters, words less than 4 in length\n",
    "# 3. convert to lower case\n",
    "# 4. Lemmatize the words\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def clean_text(text) :\n",
    "    \n",
    "    # Clean text\n",
    "    doc = nlp(text)\n",
    "     \n",
    "   # Tokenize using the cleaning criteria \n",
    "    tokenlist = [word.lemma_ for word in doc       \\\n",
    "                             if not word.is_stop   \\\n",
    "                            and not word.is_punct  \\\n",
    "                            and word.is_alpha      \\\n",
    "                            and len(word) > 4           ]\n",
    "    \n",
    "    tokenlist = [word for word in tokenlist        \\\n",
    "                             if len(word) > 4         ]\n",
    "         \n",
    "    return(tokenlist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning using NLTK (Not used)\n",
    "\n",
    "# 1. Word Tokenize text\n",
    "# 2. Ignore stopwords and non alpha characters, words less than 4 in length\n",
    "# 3. conver to lower case\n",
    "# 4. Lemmatize the words\n",
    "\n",
    "def clean_text_nltk(text) :\n",
    "    \n",
    "    # Clean text\n",
    "        \n",
    "    # Stop words set\n",
    "    ignorewords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer().lemmatize\n",
    "\n",
    "    # tokenize words after cleanup\n",
    "    tokenlist = [lemmatizer(word.lower()) for word in word_tokenize(text) \\\n",
    "                       if word not in ignorewords and word.isalpha() and len(word) > 4 ] \n",
    "            \n",
    "    return(tokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus\n",
    "\n",
    "# 1. Update/Add to Dictionary\n",
    "# 2. Create Bag of Words\n",
    "\n",
    "def convert_to_corpus(texts, dictionary) :\n",
    "    dictionary.add_documents(texts)    \n",
    "    corpus = [dictionary.doc2bow(text, allow_update=True) for text in texts]\n",
    "    return(corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text document folder\n",
    "folder = 'speeches'\n",
    "\n",
    "# Directory to save dictionary and corpus\n",
    "savedir = os.path.join(os.getcwd(),'tmp')\n",
    "if not os.path.exists(savedir) :\n",
    "    os.makedirs(savedir)\n",
    "\n",
    "#define dictionary\n",
    "dictionary = corpora.Dictionary()\n",
    "\n",
    "# Read, clean and convert the speeches in to corpus. Save corpus and dictionary\n",
    "\n",
    "# Read each folder\n",
    "for fld in  os.listdir(folder) : \n",
    "    \n",
    "    # Clean and convert the speeches into tokens\n",
    "    texts = list()\n",
    "    for file in os.listdir(folder + os.sep + fld) :\n",
    "        txtfile = folder + os.sep + fld + os.sep + file        \n",
    "        with open(txtfile, 'r',errors='ignore') as fobj :        \n",
    "            texts.append(clean_text(fobj.read()))\n",
    "    \n",
    "    # Create bigrams of tokens\n",
    "    bigram = models.Phrases(texts, min_count=5, threshold=100)    \n",
    "    bigram_mod = models.phrases.Phraser(bigram)    \n",
    "    texts = [bigram_mod[text] for text in texts]    \n",
    "    \n",
    "    # Create corpus and update dictionary\n",
    "    corpus, dictionary = convert_to_corpus(texts, dictionary)\n",
    "    \n",
    "    # Save the corpus\n",
    "    corpus_dir = os.path.join(savedir,fld)\n",
    "    if not os.path.exists(corpus_dir) :\n",
    "        os.makedirs(corpus_dir)    \n",
    "    corpora.MmCorpus.serialize(corpus_dir + os.sep + 'corpus.mm',corpus)\n",
    "    \n",
    "\n",
    "# Store the dictionary\n",
    "dictionary.save(savedir + '/corpus_dict.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = 'Religion israeli is religious Israel'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
