{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling - Data cleaning\n",
    "\n",
    "We have downloaded some speeches by B. Obama and M. Romney made during US Presidential election 2012. The speeches are uploaded on GitHUB and we would try to find some imortant topics each of their speeches consisted of. We will do Topic modelling using Gensim and visualize these topics using a wonderful package called pyLDAvis.\n",
    "\n",
    "In this notebook - we will concentrate on cleaning the text which is very important steps towards modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\envs\\py35\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Find folders and text under a given path\n",
    "import os\n",
    "\n",
    "# NLTK package\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# 1. Word Tokenize text\n",
    "# 2. Ignore stopwords and non alpha characters, words less than 4 in length\n",
    "# 3. conver to lower case\n",
    "# 4. Lemmatize the words\n",
    "\n",
    "def clean_text(text) :\n",
    "    \n",
    "    # Clean text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Stop words set\n",
    "    #ignorewords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Lemmatizer\n",
    "    #lemmatizer = WordNetLemmatizer().lemmatize\n",
    "\n",
    "    # tokenize words after cleanup\n",
    "    #tokenlist = [lemmatizer(word.lower()) for word in word_tokenize(text) \\\n",
    "    #                   if word not in ignorewords and word.isalpha() and len(word) > 4 ] \n",
    "    \n",
    "    tokenlist = [word.lemma_ for word in doc       \\\n",
    "                             if not word.is_stop   \\\n",
    "                            and not word.is_punct  \\\n",
    "                            and word.is_alpha    ]\n",
    "        \n",
    "    return(tokenlist)\n",
    "\n",
    "\n",
    "\n",
    "def clean_text_nltk(text) :\n",
    "    \n",
    "    # Clean text\n",
    "        \n",
    "    # Stop words set\n",
    "    ignorewords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer().lemmatize\n",
    "\n",
    "    # tokenize words after cleanup\n",
    "    tokenlist = [lemmatizer(word.lower()) for word in word_tokenize(text) \\\n",
    "                       if word not in ignorewords and word.isalpha() and len(word) > 4 ] \n",
    "            \n",
    "    return(tokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus\n",
    "\n",
    "# 1. Update/Add to Dictionary\n",
    "# 2. Create Bag of Words\n",
    "\n",
    "def convert_to_corpus(texts, dictionary) :\n",
    "    dictionary.add_documents(texts)    \n",
    "    corpus = [dictionary.doc2bow(text, allow_update=True) for text in texts]\n",
    "    return(corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text document folder\n",
    "folder = 'speeches'\n",
    "\n",
    "# Directory to save dictionary and corpus\n",
    "savedir = os.path.join(os.getcwd(),'tmp')\n",
    "if not os.path.exists(savedir) :\n",
    "    os.makedirs(savedir)\n",
    "\n",
    "#define dictionary\n",
    "dictionary = corpora.Dictionary()\n",
    "\n",
    "# Read, clean and convert the speeches in to corpus. Save corpus and dictionary\n",
    "for fld in  os.listdir(folder) :       \n",
    "    texts = list()\n",
    "    for file in os.listdir(folder + os.sep + fld) :\n",
    "        txtfile = folder + os.sep + fld + os.sep + file        \n",
    "        with open(txtfile, 'r',errors='ignore') as fobj :        \n",
    "            texts.append(clean_text(fobj.read()))\n",
    "    bigram = models.Phrases(texts, min_count=5, threshold=100)    \n",
    "    bigram_mod = models.phrases.Phraser(bigram)    \n",
    "    texts = [bigram_mod[text] for text in texts]    \n",
    "    corpus, dictionary = convert_to_corpus(texts, dictionary)\n",
    "    corpus_dir = os.path.join(savedir,fld)\n",
    "    if not os.path.exists(corpus_dir) :\n",
    "        os.makedirs(corpus_dir)\n",
    "    # Save the corpus\n",
    "    corpora.MmCorpus.serialize(corpus_dir + os.sep + 'corpus.mm',corpus)\n",
    "    \n",
    "\n",
    "# Store the dictionary\n",
    "dictionary.save(savedir + '/corpus_dict.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
